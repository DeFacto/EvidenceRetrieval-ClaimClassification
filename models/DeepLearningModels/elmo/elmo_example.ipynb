{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Activation, Dropout\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3859228491783142"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo = ElmoEmbedder()\n",
    "\n",
    "# token1 = [\"data mining\"]\n",
    "# token2 = [\"zxcxzc\"]\n",
    "\n",
    "# vector1 = elmo.embed_sentence(token1)\n",
    "# vector2 = elmo.embed_sentence(token2)\n",
    "\n",
    "# scipy.spatial.distance.cosine(vector1[0], vector2[1])\n",
    "\n",
    "tokens = [\"data mining\"]\n",
    "vectors = elmo.embed_sentence(tokens)\n",
    "\n",
    "vectors2 = elmo.embed_sentence([\"aerospace engineering\"])\n",
    "scipy.spatial.distance.cosine(vectors[2][0], vectors2[2][0]) # cosine distance between \"apple\" and \"carrot\" in the last layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.0.176'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# print (torch.__version__)\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files from a directory into dictionaries\n",
    "def load_directory_data(directory, label):\n",
    "    data = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data.append({\"text\": f.read().replace(\"<br />\", \" \"), \"label\": label})\n",
    "    return data\n",
    "\n",
    "# Load the positive and negative examples from the dataset\n",
    "def load_dataset(directory):\n",
    "    pos_data = load_directory_data(os.path.join(directory, \"pos\"), 1)\n",
    "    neg_data = load_directory_data(os.path.join(directory, \"neg\"), 0)\n",
    "    return pos_data+neg_data\n",
    "\n",
    "# Download and process the IMDB dataset\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "\n",
    "    train_data = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
    "    test_data = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = download_and_load_datasets()\n",
    "\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The first \"side-story\" in the universal century Gundam universe presents a refreshing new look at the war between earth and the space colonies. The focus is no longer on a small group of individuals who would go on to play pivotal roles in the conflict, but on the everyday civilian population and how the war is seen through their eyes.  The story does contain some Gundam staples, its premise being the attempts by a ZEON squad to capture an experimental Gundam, but it the execution of the plot that made this show so interesting to watch. This series focuses on the experiences of a young boy named Alfred and the relationship between his neighbor, Christina Mckenzie who is secretly a Federation pilot and a newbie Zeon pilot named Bernie Wiseman. Alfred develops a sort of \"brotherly love\" for Bernie while our young Zeon pilot also falls for Christina.  \"War in the Pocket\" proves that you do not need a sweeping epic tale about special individuals to make for a good war story. There are no uber ace pilots or large scale fleet battles to be seen here. This short 6 episode OVA focuses a lot more on character emotional drama over other themes like politics or philosophy and i love how realistically portrayed the characters are. Alfred is your typical everyday kid who plays violent computer games and thinks the armed forces is cool. He is then given a crash course in the horrible realities of war. The unlikely friendship and bonding between Bernie and Christina, each not knowing the fact that they are soldiers on different sides of the war, is played very real without going overboard with the romance drama stuff. Same goes for the endearing relationship between Alfred and Bernie. That being said, i would not want to spoil much of the story here, but it makes it a whole lot more heart wrenching to watch the tragedies that unfold as the show moves along all the way to its emotionally devastating twist ending.   Despite its lack of action, this show never falls into the category of \"boring\". The characters are just that engaging enough to carry the whole show. Not to worry as there are a number of mobile suit action scenes scattered here and there. Each are beautifully animated on a level that surpasses that of an OVA and are sure to satisfy the craving for some \"mandatory\" mobile suit battles in a Gundam series.  Normally watching anime in Japanese or English, i would leave up to personal preference. But in this case, i strongly recommend the English voice track over the Japanese one. Not only do the characters, whom all except Alfred are caucasian, sound more believable in English but the performances of the English voice cast are on par and even surpass the Japanese one, instilling each character with such realistic emotions and intonations that they sound just the acting in some live action TV dramas.  In short, this show does not try to impress the audience. What it does is conveys numerous heartwarming themes that hit closest to home especially the death of innocence on the battlefield and the horrors of war through the eyes of a child. A truly moving little story that deserves more credit than it is being given.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text. Note, it would be better to first split it into sentences.\n",
    "def tokenize_text(documents, max_tokens):\n",
    "    for document in documents:\n",
    "        document['tokens'] = keras.preprocessing.text.text_to_word_sequence(document['text'], lower=False)\n",
    "        document['tokens'] = document['tokens'][0:max_tokens]\n",
    " \n",
    "max_tokens = 100\n",
    "tokenize_text(train_data, max_tokens)\n",
    "tokenize_text(test_data, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.  One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).  One might better spend one's time staring out a window at a tree growing.  \", 'label': 0, 'tokens': ['If', 'only', 'to', 'avoid', 'making', 'this', 'type', 'of', 'film', 'in', 'the', 'future', 'This', 'film', 'is', 'interesting', 'as', 'an', 'experiment', 'but', 'tells', 'no', 'cogent', 'story', 'One', 'might', 'feel', 'virtuous', 'for', 'sitting', 'thru', 'it', 'because', 'it', 'touches', 'on', 'so', 'many', 'IMPORTANT', 'issues', 'but', 'it', 'does', 'so', 'without', 'any', 'discernable', 'motive', 'The', 'viewer', 'comes', 'away', 'with', 'no', 'new', 'perspectives', 'unless', 'one', 'comes', 'up', 'with', 'one', 'while', \"one's\", 'mind', 'wanders', 'as', 'it', 'will', 'invariably', 'do', 'during', 'this', 'pointless', 'film', 'One', 'might', 'better', 'spend', \"one's\", 'time', 'staring', 'out', 'a', 'window', 'at', 'a', 'tree', 'growing']}\n"
     ]
    }
   ],
   "source": [
    "print (train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ":: Lookup of 1000 ELMo representations. This takes a while ::\n",
      "100%[==================================================] 1000/1000 sentences\n",
      "\n",
      ":: Lookup of 1000 ELMo representations. This takes a while ::\n",
      "100%[==================================================] 1000/1000 sentences"
     ]
    }
   ],
   "source": [
    "# Lookup the ELMo embeddings for all documents (all sentences) in our dataset. Store those\n",
    "# in a numpy matrix so that we must compute the ELMo embeddings only once.\n",
    "def create_elmo_embeddings(elmo, documents, max_sentences = 1000):\n",
    "    \n",
    "    num_sentences = min(max_sentences, len(documents)) if max_sentences > 0 else len(documents)\n",
    "    print(\"\\n\\n:: Lookup of \"+str(num_sentences)+\" ELMo representations. This takes a while ::\")\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    tokens = [document['tokens'] for document in documents]\n",
    "    \n",
    "    documentIdx = 0\n",
    "    for elmo_embedding in elmo.embed_sentences(tokens):  \n",
    "        document = documents[documentIdx]\n",
    "        # Average the 3 layers returned from ELMo\n",
    "        avg_elmo_embedding = np.average(elmo_embedding, axis=0)\n",
    "             \n",
    "        embeddings.append(avg_elmo_embedding)        \n",
    "        labels.append(document['label'])\n",
    "            \n",
    "        # Some progress info\n",
    "        documentIdx += 1\n",
    "        percent = 100.0 * documentIdx / num_sentences\n",
    "        line = '[{0}{1}]'.format('=' * int(percent / 2), ' ' * (50 - int(percent / 2)))\n",
    "        status = '\\r{0:3.0f}%{1} {2:3d}/{3:3d} sentences'\n",
    "        sys.stdout.write(status.format(percent, line, documentIdx, num_sentences))\n",
    "        \n",
    "        if max_sentences > 0 and documentIdx >= max_sentences:\n",
    "            break\n",
    "            \n",
    "    return embeddings, labels\n",
    "\n",
    "\n",
    "elmo = ElmoEmbedder(cuda_device=0) #Set cuda_device to the ID of your GPU if you have one\n",
    "train_x, train_y = create_elmo_embeddings(elmo, train_data, 1000)\n",
    "test_x, test_y  = create_elmo_embeddings(elmo, test_data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print (len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Train X: (1000, 100, 1024)\n",
      "Shape Test Y: (1000, 100, 1024)\n"
     ]
    }
   ],
   "source": [
    "# :: Pad the x matrix to uniform length ::\n",
    "def pad_x_matrix(x_matrix):\n",
    "    for sentenceIdx in range(len(x_matrix)):\n",
    "        sent = x_matrix[sentenceIdx]\n",
    "        sentence_vec = np.array(sent, dtype=np.float32)\n",
    "        padding_length = max_tokens - sentence_vec.shape[0]\n",
    "        if padding_length > 0:\n",
    "            x_matrix[sentenceIdx] = np.append(sent, np.zeros((padding_length, sentence_vec.shape[1])), axis=0)\n",
    "\n",
    "    matrix = np.array(x_matrix, dtype=np.float32)\n",
    "    return matrix\n",
    "\n",
    "train_x = pad_x_matrix(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "test_x = pad_x_matrix(test_x)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "print(\"Shape Train X:\", train_x.shape)\n",
    "print(\"Shape Test Y:\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7980 - acc: 0.6110 - val_loss: 0.6037 - val_acc: 0.6690\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.3522 - acc: 0.8720 - val_loss: 0.4799 - val_acc: 0.7700\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.1340 - acc: 0.9820 - val_loss: 0.4727 - val_acc: 0.7870\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.5149 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 0.7850\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.5332 - val_acc: 0.7880\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5536 - val_acc: 0.7900\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.5786 - val_acc: 0.7870\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5851 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.5921 - val_acc: 0.7860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc57f135c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple model for sentence / document classification using CNN + global max pooling\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=250, kernel_size=3, padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
